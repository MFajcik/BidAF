## Model performances
### Baseline performance [no emb training]
\#parameters = 4 333 200  
"exact_match": 23.926206244087037   
 "f1": 37.63605082140957  

### Baseline performance [with emb]
89712 Vocab size  
\#parameters = 31 246 800  

(probably overfitting)  
"exact_match": 19.5837275307474,   
"f1": 32.03623208281153

### Bidaf [only GloVe]
\# parameters 1 370 605  
"exact_match": 41.06906338694418,   
"f1": 58.64649994567137


### Vanilla bidaf:
\# params: 1868881

Tensor sizes:  
{
   'char_embedder.embeddings.weight':11376,
   'char_embedder.char_conv.weight':4000,
   'char_embedder.char_conv.bias':100,
   'highway_network.highway_linear0.0.weight':40000,
   'highway_network.highway_linear0.0.bias':200,
   'highway_network.highway_gate0.0.weight':40000,
   'highway_network.highway_gate0.0.bias':200,
   'highway_network.highway_linear1.0.weight':40000,
   'highway_network.highway_linear1.0.bias':200,
   'highway_network.highway_gate1.0.weight':40000,
   'highway_network.highway_gate1.0.bias':200,
   'encoderQ.hidden_params':200,
   'encoderQ.cell_params':200,
   'encoderQ.rnn.weight_ih_l0':80000,
   'encoderQ.rnn.weight_hh_l0':40000,
   'encoderQ.rnn.bias_ih_l0':400,
   'encoderQ.rnn.bias_hh_l0':400,
   'encoderQ.rnn.weight_ih_l0_reverse':80000,
   'encoderQ.rnn.weight_hh_l0_reverse':40000,
   'encoderQ.rnn.bias_ih_l0_reverse':400,
   'encoderQ.rnn.bias_hh_l0_reverse':400,
   'encoderD.hidden_params':200,
   'encoderD.cell_params':200,
   'encoderD.rnn.weight_ih_l0':80000,
   'encoderD.rnn.weight_hh_l0':40000,
   'encoderD.rnn.bias_ih_l0':400,
   'encoderD.rnn.bias_hh_l0':400,
   'encoderD.rnn.weight_ih_l0_reverse':80000,
   'encoderD.rnn.weight_hh_l0_reverse':40000,
   'encoderD.rnn.bias_ih_l0_reverse':400,
   'encoderD.rnn.bias_hh_l0_reverse':400,
   'lin_S.weight':1000,
   'lin_S.bias':1,
   'lin_E.weight':1000,
   'lin_E.bias':1,
   'att_weight_c.weight':200,
   'att_weight_c.bias':1,
   'att_weight_q.weight':200,
   'att_weight_q.bias':1,
   'att_weight_cq.weight':200,
   'att_weight_cq.bias':1,
   'modeling_layer.hidden_params':400,
   'modeling_layer.cell_params':400,
   'modeling_layer.rnn.weight_ih_l0':320000,
   'modeling_layer.rnn.weight_hh_l0':40000,
   'modeling_layer.rnn.bias_ih_l0':400,
   'modeling_layer.rnn.bias_hh_l0':400,
   'modeling_layer.rnn.weight_ih_l0_reverse':320000,
   'modeling_layer.rnn.weight_hh_l0_reverse':40000,
   'modeling_layer.rnn.bias_ih_l0_reverse':400,
   'modeling_layer.rnn.bias_hh_l0_reverse':400,
   'modeling_layer.rnn.weight_ih_l1':80000,
   'modeling_layer.rnn.weight_hh_l1':40000,
   'modeling_layer.rnn.bias_ih_l1':400,
   'modeling_layer.rnn.bias_hh_l1':400,
   'modeling_layer.rnn.weight_ih_l1_reverse':80000,
   'modeling_layer.rnn.weight_hh_l1_reverse':40000,
   'modeling_layer.rnn.bias_ih_l1_reverse':400,
   'modeling_layer.rnn.bias_hh_l1_reverse':400,
   'output_layer.hidden_params':200,
   'output_layer.cell_params':200,
   'output_layer.rnn.weight_ih_l0':80000,
   'output_layer.rnn.weight_hh_l0':40000,
   'output_layer.rnn.bias_ih_l0':400,
   'output_layer.rnn.bias_hh_l0':400,
   'output_layer.rnn.weight_ih_l0_reverse':80000,
   'output_layer.rnn.weight_hh_l0_reverse':40000,
   'output_layer.rnn.bias_ih_l0_reverse':400,
   'output_layer.rnn.bias_hh_l0_reverse':400
}
{'char_embedder.embeddings.weight': torch.Size([1422, 8]), 
'char_embedder.char_conv.weight': torch.Size([100, 1, 8, 5]), 
'char_embedder.char_conv.bias': torch.Size([100]), 
'highway_network.highway_linear0.0.weight': torch.Size([200, 200]), 
'highway_network.highway_linear0.0.bias': torch.Size([200]), 'highway_network.highway_gate0.0.weight': torch.Size([200, 200]), 'highway_network.highway_gate0.0.bias': torch.Size([200]), 'highway_network.highway_linear1.0.weight': torch.Size([200, 200]), 'highway_network.highway_linear1.0.bias': torch.Size([200]), 'highway_network.highway_gate1.0.weight': torch.Size([200, 200]), 'highway_network.highway_gate1.0.bias': torch.Size([200]), 'encoderQ.hidden_params': torch.Size([2, 1, 100]), 'encoderQ.cell_params': torch.Size([2, 1, 100]), 'encoderQ.rnn.weight_ih_l0': torch.Size([400, 200]), 'encoderQ.rnn.weight_hh_l0': torch.Size([400, 100]), 'encoderQ.rnn.bias_ih_l0': torch.Size([400]), 'encoderQ.rnn.bias_hh_l0': torch.Size([400]), 'encoderQ.rnn.weight_ih_l0_reverse': torch.Size([400, 200]), 'encoderQ.rnn.weight_hh_l0_reverse': torch.Size([400, 100]), 'encoderQ.rnn.bias_ih_l0_reverse': torch.Size([400]), 'encoderQ.rnn.bias_hh_l0_reverse': torch.Size([400]), 'encoderD.hidden_params': torch.Size([2, 1, 100]), 'encoderD.cell_params': torch.Size([2, 1, 100]), 'encoderD.rnn.weight_ih_l0': torch.Size([400, 200]), 'encoderD.rnn.weight_hh_l0': torch.Size([400, 100]), 'encoderD.rnn.bias_ih_l0': torch.Size([400]), 'encoderD.rnn.bias_hh_l0': torch.Size([400]), 'encoderD.rnn.weight_ih_l0_reverse': torch.Size([400, 200]), 'encoderD.rnn.weight_hh_l0_reverse': torch.Size([400, 100]), 'encoderD.rnn.bias_ih_l0_reverse': torch.Size([400]), 'encoderD.rnn.bias_hh_l0_reverse': torch.Size([400]), 'lin_S.weight': torch.Size([1, 1000]), 'lin_S.bias': torch.Size([1]), 'lin_E.weight': torch.Size([1, 1000]), 'lin_E.bias': torch.Size([1]), 'att_weight_c.weight': torch.Size([1, 200]), 'att_weight_c.bias': torch.Size([1]), 'att_weight_q.weight': torch.Size([1, 200]), 'att_weight_q.bias': torch.Size([1]), 'att_weight_cq.weight': torch.Size([1, 200]), 'att_weight_cq.bias': torch.Size([1]), 'modeling_layer.hidden_params': torch.Size([4, 1, 100]), 'modeling_layer.cell_params': torch.Size([4, 1, 100]), 'modeling_layer.rnn.weight_ih_l0': torch.Size([400, 800]), 'modeling_layer.rnn.weight_hh_l0': torch.Size([400, 100]), 'modeling_layer.rnn.bias_ih_l0': torch.Size([400]), 'modeling_layer.rnn.bias_hh_l0': torch.Size([400]), 'modeling_layer.rnn.weight_ih_l0_reverse': torch.Size([400, 800]), 'modeling_layer.rnn.weight_hh_l0_reverse': torch.Size([400, 100]), 'modeling_layer.rnn.bias_ih_l0_reverse': torch.Size([400]), 'modeling_layer.rnn.bias_hh_l0_reverse': torch.Size([400]), 'modeling_layer.rnn.weight_ih_l1': torch.Size([400, 200]), 'modeling_layer.rnn.weight_hh_l1': torch.Size([400, 100]), 'modeling_layer.rnn.bias_ih_l1': torch.Size([400]), 'modeling_layer.rnn.bias_hh_l1': torch.Size([400]), 'modeling_layer.rnn.weight_ih_l1_reverse': torch.Size([400, 200]), 'modeling_layer.rnn.weight_hh_l1_reverse': torch.Size([400, 100]), 'modeling_layer.rnn.bias_ih_l1_reverse': torch.Size([400]), 'modeling_layer.rnn.bias_hh_l1_reverse': torch.Size([400]), 'output_layer.hidden_params': torch.Size([2, 1, 100]), 'output_layer.cell_params': torch.Size([2, 1, 100]), 'output_layer.rnn.weight_ih_l0': torch.Size([400, 200]), 'output_layer.rnn.weight_hh_l0': torch.Size([400, 100]), 'output_layer.rnn.bias_ih_l0': torch.Size([400]), 'output_layer.rnn.bias_hh_l0': torch.Size([400]), 'output_layer.rnn.weight_ih_l0_reverse': torch.Size([400, 200]), 'output_layer.rnn.weight_hh_l0_reverse': torch.Size([400, 100]), 'output_layer.rnn.bias_ih_l0_reverse': torch.Size([400]), 'output_layer.rnn.bias_hh_l0_reverse': torch.Size([400])}

### AllenNLP bidaf:  
\# params: 2582095

Tensor sizes:  
{'_text_field_embedder.token_embedder_token_characters._embedding._module.weight': 4192,
 '_text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight': 8000,
 '_text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias': 100,
 '_highway_layer._module._layers.0.weight': 80000,
 '_highway_layer._module._layers.0.bias': 400,
 '_highway_layer._module._layers.1.weight': 80000,
 '_highway_layer._module._layers.1.bias': 400,
 '_phrase_layer._module.weight_ih_l0': 80000,
 '_phrase_layer._module.weight_hh_l0': 40000,
 '_phrase_layer._module.bias_ih_l0': 400,
 '_phrase_layer._module.bias_hh_l0': 400,
 '_phrase_layer._module.weight_ih_l0_reverse': 80000,
 '_phrase_layer._module.weight_hh_l0_reverse': 40000,
 '_phrase_layer._module.bias_ih_l0_reverse': 400,
 '_phrase_layer._module.bias_hh_l0_reverse': 400,
 '_matrix_attention._similarity_function._weight_vector': 600,
 '_matrix_attention._similarity_function._bias': 1,
 '_modeling_layer._module.weight_ih_l0': 320000,
 '_modeling_layer._module.weight_hh_l0': 40000,
 '_modeling_layer._module.bias_ih_l0': 400,
 '_modeling_layer._module.bias_hh_l0': 400,
 '_modeling_layer._module.weight_ih_l0_reverse': 320000,
 '_modeling_layer._module.weight_hh_l0_reverse': 40000,
 '_modeling_layer._module.bias_ih_l0_reverse': 400,
 '_modeling_layer._module.bias_hh_l0_reverse': 400,
 '_modeling_layer._module.weight_ih_l1': 80000,
 '_modeling_layer._module.weight_hh_l1': 40000,
 '_modeling_layer._module.bias_ih_l1': 400,
 '_modeling_layer._module.bias_hh_l1': 400,
 '_modeling_layer._module.weight_ih_l1_reverse': 80000,
 '_modeling_layer._module.weight_hh_l1_reverse': 40000,
 '_modeling_layer._module.bias_ih_l1_reverse': 400,
 '_modeling_layer._module.bias_hh_l1_reverse': 400,
 '_span_end_encoder._module.weight_ih_l0': 560000,
 '_span_end_encoder._module.weight_hh_l0': 40000,
 '_span_end_encoder._module.bias_ih_l0': 400,
 '_span_end_encoder._module.bias_hh_l0': 400,
 '_span_end_encoder._module.weight_ih_l0_reverse': 560000,
 '_span_end_encoder._module.weight_hh_l0_reverse': 40000,
 '_span_end_encoder._module.bias_ih_l0_reverse': 400,
 '_span_end_encoder._module.bias_hh_l0_reverse': 400,
 '_span_start_predictor._module.weight': 1000,
 '_span_start_predictor._module.bias': 1,
 '_span_end_predictor._module.weight': 1000,
 '_span_end_predictor._module.bias': 1}
 
 {'_text_field_embedder.token_embedder_token_characters._embedding._module.weight': torch.Size([262, 16]),
 '_text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight': torch.Size([100, 16, 5]),
 '_text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias': torch.Size([100]),
 '_highway_layer._module._layers.0.weight': torch.Size([400, 200]),
 '_highway_layer._module._layers.0.bias': torch.Size([400]),
 '_highway_layer._module._layers.1.weight': torch.Size([400, 200]),
 '_highway_layer._module._layers.1.bias': torch.Size([400]),
 '_phrase_layer._module.weight_ih_l0': torch.Size([400, 200]),
 '_phrase_layer._module.weight_hh_l0': torch.Size([400, 100]),
 '_phrase_layer._module.bias_ih_l0': torch.Size([400]),
 '_phrase_layer._module.bias_hh_l0': torch.Size([400]),
 '_phrase_layer._module.weight_ih_l0_reverse': torch.Size([400, 200]),
 '_phrase_layer._module.weight_hh_l0_reverse': torch.Size([400, 100]),
 '_phrase_layer._module.bias_ih_l0_reverse': torch.Size([400]),
 '_phrase_layer._module.bias_hh_l0_reverse': torch.Size([400]),
 '_matrix_attention._similarity_function._weight_vector': torch.Size([600]),
 '_matrix_attention._similarity_function._bias': torch.Size([1]),
 '_modeling_layer._module.weight_ih_l0': torch.Size([400, 800]),
 '_modeling_layer._module.weight_hh_l0': torch.Size([400, 100]),
 '_modeling_layer._module.bias_ih_l0': torch.Size([400]),
 '_modeling_layer._module.bias_hh_l0': torch.Size([400]),
 '_modeling_layer._module.weight_ih_l0_reverse': torch.Size([400, 800]),
 '_modeling_layer._module.weight_hh_l0_reverse': torch.Size([400, 100]),
 '_modeling_layer._module.bias_ih_l0_reverse': torch.Size([400]),
 '_modeling_layer._module.bias_hh_l0_reverse': torch.Size([400]),
 '_modeling_layer._module.weight_ih_l1': torch.Size([400, 200]),
 '_modeling_layer._module.weight_hh_l1': torch.Size([400, 100]),
 '_modeling_layer._module.bias_ih_l1': torch.Size([400]),
 '_modeling_layer._module.bias_hh_l1': torch.Size([400]),
 '_modeling_layer._module.weight_ih_l1_reverse': torch.Size([400, 200]),
 '_modeling_layer._module.weight_hh_l1_reverse': torch.Size([400, 100]),
 '_modeling_layer._module.bias_ih_l1_reverse': torch.Size([400]),
 '_modeling_layer._module.bias_hh_l1_reverse': torch.Size([400]),
 '_span_end_encoder._module.weight_ih_l0': torch.Size([400, 1400]),
 '_span_end_encoder._module.weight_hh_l0': torch.Size([400, 100]),
 '_span_end_encoder._module.bias_ih_l0': torch.Size([400]),
 '_span_end_encoder._module.bias_hh_l0': torch.Size([400]),
 '_span_end_encoder._module.weight_ih_l0_reverse': torch.Size([400, 1400]),
 '_span_end_encoder._module.weight_hh_l0_reverse': torch.Size([400, 100]),
 '_span_end_encoder._module.bias_ih_l0_reverse': torch.Size([400]),
 '_span_end_encoder._module.bias_hh_l0_reverse': torch.Size([400]),
 '_span_start_predictor._module.weight': torch.Size([1, 1000]),
 '_span_start_predictor._module.bias': torch.Size([1]),
 '_span_end_predictor._module.weight': torch.Size([1, 1000]),
 '_span_end_predictor._module.bias': torch.Size([1])}